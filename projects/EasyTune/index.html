<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="description" content="EasyTune: Efficient Step-Aware Reinforcement Fine-Tuning for Diffusion-Based Motion Generation">
  <meta property="og:title" content="EasyTune: Efficient Step-Aware Reinforcement Fine-Tuning for Diffusion-Based Motion Generation"/>
  <meta property="og:description" content="A reinforcement fine-tuning framework for diffusion models that decouples recursive dependencies and enables dense optimization, memory-efficient training, and fine-grained alignment."/>
  <meta property="og:url" content="https://xiaofeng-tan.github.io/projects/EasyTune/"/>
  <meta property="og:image" content="static/images/framework.png" />
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="Motion Generation, Diffusion Model, Fine-Tuning, Deep Learning, Human Motion">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="author" content="Xiaofeng Tan">

  <title>EasyTune | Motion Generation Reinforcement Fine-Tuning</title>
  <link rel="icon" type="image/svg+xml" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>üèÉ</text></svg>">
  
  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif:ital,wght@0,400;0,500;0,600;0,700;1,400&family=Noto+Sans:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
  
  <!-- Stylesheets -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
</head>
<body>

<!-- Hero Header -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">
            <span class="method-name">EasyTune</span>: Efficient Step-Aware Reinforcement Fine-Tuning<br>for Diffusion-Based Motion Generation
          </h1>
          
          <!-- Authors -->
          <div class="publication-authors">
            <span class="author-block"><a href="https://xiaofeng-tan.github.io/" target="_blank">Xiaofeng Tan</a><sup>*</sup>,</span>
            <span class="author-block">Wanjiang Weng<sup>*</sup>,</span>
            <span class="author-block">Haodong Lei<sup></sup>,</span>
            <span class="author-block">Hongsong Wang<sup>‚Ä†</sup></span>
          </div>
          <div class="publication-affiliations">
            <span class="affiliation-block"><a href="https://palm.seu.edu.cn/" target="_blank">PALM Lab</a>, Southeast University</span>
          </div>
          <div class="author-notes">
            <span><sup>*</sup>Equal contribution</span>
            <span><sup>‚Ä†</sup>Corresponding author</span>
          </div>
          <div class="contact-info">
            <span>For any questions, please contact <a href="mailto:xiaofengtan@seu.edu.cn">xiaofengtan@seu.edu.cn</a> or visit <a href="https://xiaofeng-tan.github.io/" target="_blank">my homepage</a>.</span>
          </div>
          
          <!-- TL;DR -->
          <div class="tldr-container">
            <p><strong>üí° TL;DR:</strong> We propose <strong>EasyTune</strong>, a reinforcement fine-tuning framework for diffusion models that decouples recursive dependencies and enables <strong>(1)</strong> dense and effective optimization, <strong>(2)</strong> memory-efficient training, and <strong>(3)</strong> fine-grained alignment.</p>
          </div>
          
          <!-- Links -->
          <div class="publication-links">
            <a href="https://openreview.net/forum?id=Fy1EoIaAzQ" target="_blank" class="button btn-openreview">
              <span class="icon"><img src="https://openreview.net/favicon.ico" alt="OpenReview" class="btn-icon"></span>
              <span>OpenReview</span>
            </a>
            <a href="#" target="_blank" class="button btn-arxiv">
              <span class="icon"><i class="ai ai-arxiv"></i></span>
              <span>arXiv</span>
            </a>
            <a href="#" target="_blank" class="button btn-code">
              <span class="icon"><i class="fab fa-github"></i></span>
              <span>Code</span>
            </a>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Performance Highlight -->
<section class="section highlight-section">
  <div class="container is-max-desktop">
    <div class="content-block">
      <h2 class="section-title">üöÄ Performance</h2>
      
      <div class="stats-grid">
        <div class="stat-card">
          <div class="stat-number">62.1%</div>
          <div class="stat-label">MM-Dist Improvement</div>
        </div>
        <div class="stat-card">
          <div class="stat-number">34.5%</div>
          <div class="stat-label">Memory Overhead</div>
        </div>
        <div class="stat-card">
          <div class="stat-number">6√ó</div>
          <div class="stat-label">Faster Training</div>
        </div>
      </div>
      
      <div class="figure-container">
        <div class="figure-wrapper">
          <img src="static/images/intro.png" alt="Performance comparison"/>
        </div>
        <p class="figure-caption">
          <strong>Figure 1.</strong> Comparison of training costs and generation performance on HumanML3D. (a) Performance comparison of different fine-tuning methods. (b) Generalization performance across six pre-trained diffusion-based models.
        </p>
      </div>
    </div>
  </div>
</section>

<!-- Abstract -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="content-block">
      <h2 class="section-title">üìù Abstract</h2>
      <p class="abstract-text">
        In recent years, motion generative models have undergone significant advancement, yet pose challenges in aligning with downstream objectives. Recent studies have shown that using differentiable rewards to directly align the preference of diffusion models yields promising results. However, these methods suffer from <strong>inefficient and coarse-grained optimization</strong> with <strong>high memory consumption</strong>. In this work, we first theoretically identify the <em>fundamental reason</em> of these limitations: the recursive dependence between different steps in the denoising trajectory. Inspired by this insight, we propose <strong>EasyTune</strong>, which fine-tunes diffusion at each denoising step rather than over the entire trajectory. This decouples the recursive dependence, allowing us to perform <strong>(1)</strong> a dense and fine-grained, and <strong>(2)</strong> memory-efficient optimization. Furthermore, the scarcity of preference motion pairs restricts the availability of motion reward model training. To this end, we further introduce a <strong>Self-refinement Preference Learning (SPL)</strong> mechanism that dynamically identifies preference pairs and conducts preference learning. Experiments show that compared with DRaFT-50, EasyTune achieves an 8.91% improvement in alignment metric (MM-Dist) while only incurring 31.16% of the additional memory overhead.
      </p>
    </div>
  </div>
</section>

<!-- Framework -->
<section class="section alt-bg">
  <div class="container is-max-desktop">
    <div class="content-block">
      <h2 class="section-title">üèóÔ∏è Framework</h2>
      
      <div class="figure-container">
        <div class="figure-wrapper">
          <img src="static/images/framework.png" alt="EasyTune Framework"/>
        </div>
        <p class="figure-caption">
          <strong>Figure 2.</strong> The framework of existing differentiable reward-based methods (left) and our proposed EasyTune (right). Existing methods backpropagate gradients through the overall denoising process, resulting in excessive memory, inefficient, and coarse-grained optimization. EasyTune optimizes by directly backpropagating gradients at each denoising step.
        </p>
      </div>
    </div>
  </div>
</section>

<!-- Core Insight -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="content-block">
      <h2 class="section-title">üí° Core Insight</h2>
      
      <div class="insight-box">
        <p>Existing differentiable reward-based methods suffer from <strong>inefficient and coarse-grained optimization</strong> with <strong>high memory consumption</strong>. We identify the fundamental reason: <em>the recursive dependence between different steps in the denoising trajectory</em>.</p>
        <p style="margin-top: 0.8rem;">This decouples the recursive dependence, allowing us to perform <strong>(1)</strong> a dense and fine-grained, and <strong>(2)</strong> memory-efficient optimization:</p>
        <ul class="insight-list">
          <li><strong>üì¶ Step-wise graph storage</strong> ‚Äî Reduced memory footprint</li>
          <li><strong>‚ö° Dense per-step optimization</strong> ‚Äî Faster convergence</li>
          <li><strong>üéØ Fine-grained parameter optimization</strong> ‚Äî Better alignment</li>
        </ul>
      </div>
      
      <div class="figure-container">
        <div class="figure-wrapper medium">
          <img src="static/images/insight.png" alt="Core Insight"/>
        </div>
        <p class="figure-caption">
          <strong>Figure 3.</strong> Core insight of EasyTune. By replacing the recursive gradient in Eq.(4) with the formulation in Eq.(7), EasyTune decouples the recursive dependence of computation graph.
        </p>
      </div>
    </div>
  </div>
</section>

<!-- Experiments -->
<section class="section alt-bg">
  <div class="container is-max-desktop">
    <div class="content-block">
      <h2 class="section-title">üìä Experiments</h2>
      
      <h3 class="subsection-title">üèÜ Main Results on HumanML3D</h3>
      <div class="figure-container">
        <div class="figure-wrapper">
          <img src="static/images/t1.png" alt="Main results"/>
        </div>
        <p class="figure-caption">
          <strong>Table 1.</strong> Comparison of SoTA fine-tuning methods on HumanML3D dataset. ‚Üë/‚Üì/‚Üí indicate higher/lower/closer-to-real values are better. <strong>Bold</strong> and <u>underline</u> highlight best and second-best results.
        </p>
      </div>
      
      <h3 class="subsection-title">üìà Text-to-Motion Generation</h3>
      <div class="figure-container">
        <div class="figure-wrapper">
          <img src="static/images/t2.png" alt="Text-to-motion results"/>
        </div>
        <p class="figure-caption">
          <strong>Table 2.</strong> Comparison of text-to-motion generation performance on the HumanML3D dataset.
        </p>
      </div>
      
      <h3 class="subsection-title">üî¨ Text-Motion Retrieval</h3>
      <div class="figure-container">
        <div class="figure-wrapper">
          <img src="static/images/t3.png" alt="Retrieval benchmark"/>
        </div>
        <p class="figure-caption">
          <strong>Table 3.</strong> Evaluation on Text-Motion Retrieval Benchmark. "Noise" indicates whether the method can handle noisy motion from the denoising process.
        </p>
      </div>
      
      <div class="figure-grid-2col">
        <div class="figure-container">
          <div class="figure-wrapper">
            <img src="static/images/t4.png" alt="Enhancement results"/>
          </div>
          <p class="figure-caption">
            <strong>Table 4.</strong> Performance enhancement of diffusion-based motion generation methods with EasyTune.
          </p>
        </div>
        <div class="figure-container">
          <div class="figure-wrapper">
            <img src="static/images/t5.png" alt="KIT-ML results"/>
          </div>
          <p class="figure-caption">
            <strong>Table 5.</strong> Comparison of SoTA fine-tuning methods on KIT-ML dataset.
          </p>
        </div>
      </div>
      
      <h3 class="subsection-title">üìâ Training Analysis</h3>
      <div class="figure-container">
        <div class="figure-wrapper">
          <img src="static/images/fig_exp.png" alt="Training analysis"/>
        </div>
        <p class="figure-caption">
          <strong>Figure 4.</strong> Loss curves for EasyTune and existing fine-tuning methods (Left). Comparison of winning rates % for diffusion models fine-tuned with and without SPL (Right).
        </p>
      </div>
    </div>
  </div>
</section>

<!-- Visualizations -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="content-block">
      <h2 class="section-title">üé® Visual Results</h2>
      
      <div class="figure-container">
        <div class="figure-wrapper large">
          <img src="static/images/vis.png" alt="Visual results"/>
        </div>
        <p class="figure-caption">
          <strong>Figure 5.</strong> Visual results on HumanML3D dataset. "w/o EasyTune" refers to motions generated by the original MLD model, while "w/ EasyTune" indicates motions generated by MLD fine-tuned using EasyTune.
        </p>
      </div>
    </div>
  </div>
</section>

<!-- Bad Case Analysis -->
<section class="section alt-bg">
  <div class="container is-max-desktop">
    <div class="content-block">
      <h2 class="section-title">‚ö†Ô∏è Bad Case Analysis: Reward Hacking</h2>
      
      <div class="insight-box">
        <p>Reward hacking is a known challenge in reinforcement learning, where continued optimization after convergence can degrade generation quality. This occurs when models over-fit to semantic alignment while neglecting realistic motion dynamics.</p>
        <p style="margin-top: 0.8rem;">As illustrated in the videos below, models misinterpret prompts to over-fit specific actions. For example, the instruction to "lifts their right foot" may result in continuous, excessive lifting. Similarly, a sequence like "squats down, then stands up and moves forward" might be incorrectly generated as "squats down while moving forward."</p>
        <p style="margin-top: 0.8rem;"><strong>Fortunately, this phenomenon can be effectively mitigated.</strong> Our method, combined with KL-divergence regularization, shows robust mitigation.</p>
      </div>
      
      <div class="hack-video-grid">
        <div class="video-item">
          <video autoplay controls muted loop playsinline>
            <source src="static/videos/hack1_title.mp4" type="video/mp4">
          </video>
        </div>
        <div class="video-item">
          <video autoplay controls muted loop playsinline>
            <source src="static/videos/hack2_title.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Demo Videos -->
<section class="section alt-bg">
  <div class="container is-max-desktop">
    <div class="content-block">
      <h2 class="section-title">üé• Motion Demos</h2>
      <p class="demo-description">Generated motion sequences comparing original vs. EasyTune fine-tuned models.</p>
      
      <div class="video-grid">
        <div class="video-item">
          <video autoplay controls muted loop playsinline>
            <source src="static/videos/192.mp4" type="video/mp4">
          </video>
        </div>
        <div class="video-item">
          <video autoplay controls muted loop playsinline>
            <source src="static/videos/247.mp4" type="video/mp4">
          </video>
        </div>
        <div class="video-item">
          <video autoplay controls muted loop playsinline>
            <source src="static/videos/307.mp4" type="video/mp4">
          </video>
        </div>
        <div class="video-item">
          <video autoplay controls muted loop playsinline>
            <source src="static/videos/444.mp4" type="video/mp4">
          </video>
        </div>
        <div class="video-item">
          <video autoplay controls muted loop playsinline>
            <source src="static/videos/1125.mp4" type="video/mp4">
          </video>
        </div>
        <div class="video-item">
          <video autoplay controls muted loop playsinline>
            <source src="static/videos/1253.mp4" type="video/mp4">
          </video>
        </div>
        <div class="video-item">
          <video autoplay controls muted loop playsinline>
            <source src="static/videos/1423.mp4" type="video/mp4">
          </video>
        </div>
        <div class="video-item">
          <video autoplay controls muted loop playsinline>
            <source src="static/videos/1535.mp4" type="video/mp4">
          </video>
        </div>
        <div class="video-item">
          <video autoplay controls muted loop playsinline>
            <source src="static/videos/2279.mp4" type="video/mp4">
          </video>
        </div>
        <div class="video-item">
          <video autoplay controls muted loop playsinline>
            <source src="static/videos/2943.mp4" type="video/mp4">
          </video>
        </div>
        <div class="video-item">
          <video autoplay controls muted loop playsinline>
            <source src="static/videos/4626.mp4" type="video/mp4">
          </video>
        </div>
        <div class="video-item">
          <video autoplay controls muted loop playsinline>
            <source src="static/videos/4646.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- BibTeX -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="content-block">
      <h2 class="section-title">üìö BibTeX</h2>
      <div class="bibtex-container">
        <button class="copy-btn" onclick="copyBibtex()">
          <i class="far fa-copy"></i> Copy
        </button>
        <pre class="bibtex"><code>@article{tan2025easytune,
  title={EasyTune: Efficient Step-Aware Reinforcement Fine-Tuning for Diffusion-Based Motion Generation},
  author={Tan, Xiaofeng and Weng, Wanjiang and Lei, Haodong and Wang, Hongsong},
  journal={arXiv preprint},
  year={2025}
}</code></pre>
      </div>
    </div>
  </div>
</section>

<!-- Footer -->
<footer class="footer">
  <div class="container">
    <p>
      Template adapted from <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
      Licensed under <a href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">CC BY-SA 4.0</a>.
    </p>
  </div>
</footer>

<script>
function copyBibtex() {
  const bibtex = document.querySelector('.bibtex code').textContent;
  navigator.clipboard.writeText(bibtex).then(() => {
    const btn = document.querySelector('.copy-btn');
    btn.innerHTML = '<i class="fas fa-check"></i> Copied';
    setTimeout(() => {
      btn.innerHTML = '<i class="far fa-copy"></i> Copy';
    }, 2000);
  });
}
</script>

</body>
</html>
