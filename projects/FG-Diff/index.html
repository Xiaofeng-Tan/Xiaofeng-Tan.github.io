<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="description" content="FG-Diff: Frequency-Guided Diffusion Model with Perturbation Training for Skeleton-Based Video Anomaly Detection">
  <meta property="og:title" content="FG-Diff: Frequency-Guided Diffusion Model for Video Anomaly Detection"/>
  <meta property="og:description" content="A novel frequency-guided diffusion model with perturbation training for skeleton-based video anomaly detection."/>
  <meta property="og:url" content="https://xiaofeng-tan.github.io/projects/FG-Diff/"/>
  <meta property="og:image" content="static/images/framework.png" />
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="Video Anomaly Detection, Diffusion Model, Skeleton-Based, Frequency Analysis, Deep Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="author" content="Xiaofeng Tan">

  <title>FG-Diff | Video Anomaly Detection</title>
  <link rel="icon" type="image/svg+xml" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>üìä</text></svg>">
  
  <link href="https://fonts.googleapis.com/css2?family=Playfair+Display:wght@500;600;700&family=Source+Serif+4:opsz,wght@8..60,400;8..60,500;8..60,600&family=Inter:wght@400;500;600&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
</head>
<body>

<!-- Hero Header -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">
            <span class="method-name">FG-Diff</span>: Frequency-Guided Diffusion Model with Perturbation Training for Skeleton-Based Video Anomaly Detection
          </h1>
          
          <div class="is-size-5 publication-authors">
            <span class="author-block"><a href="https://xiaofeng-tan.github.io/" target="_blank">Xiaofeng Tan</a><sup>1,2</sup>,</span>
            <span class="author-block"><a href="https://cs.seu.edu.cn/hongsongwang/main.htm" target="_blank">Hongsong Wang</a><sup>1,2</sup>,</span>
            <span class="author-block"><a href="https://palm.seu.edu.cn/xgeng/" target="_blank">Xin Geng</a><sup>1,2</sup>,</span>
            <span class="author-block"><a href="http://www.cbsr.ia.ac.cn/users/lwang/lwang.html" target="_blank">Liang Wang</a><sup>3,4</sup></span>
          </div>

          <div class="is-size-6 publication-affiliations">
            <span><sup>1</sup>Southeast University</span>
            <span class="aff-sep">¬∑</span>
            <span><sup>2</sup>Key Lab of New Generation AI Technology</span>
            <span class="aff-sep">¬∑</span>
            <span><sup>3</sup>NLPR & MAIS, Institute of Automation, CAS</span>
            <span class="aff-sep">¬∑</span>
            <span><sup>4</sup>UCAS</span>
          </div>
          <div class="contact-info">
            <span>For any questions, please contact <a href="mailto:xiaofengtan@seu.edu.cn">xiaofengtan@seu.edu.cn</a> or visit <a href="https://xiaofeng-tan.github.io/" target="_blank">my homepage</a>.</span>
          </div>
          
          <div class="publication-links">
            <a href="https://arxiv.org/abs/2412.03044" target="_blank" class="btn btn-paper">
              <i class="ai ai-arxiv"></i> Paper
            </a>
            <a href="https://github.com/Xiaofeng-Tan/FGDMAD-Code" target="_blank" class="btn btn-code">
              <i class="fab fa-github"></i> Code
            </a>
            <a href="https://huggingface.co/ModelsWeights/AD-FG-Diff" target="_blank" class="btn btn-model">
              <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="HF" class="hf-icon"> Model
            </a>
            <a href="https://huggingface.co/datasets/ModelsWeights/AD-FG-Diff/tree/main" target="_blank" class="btn btn-data">
              <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="HF" class="hf-icon"> Dataset
            </a>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Abstract -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="content-block">
      <h2 class="section-title">üìù Abstract</h2>
      <p class="abstract-text">
        Video anomaly detection (VAD) is a vital yet complex open-set task in computer vision, commonly tackled through reconstruction-based methods. However, these methods struggle with two key limitations: <strong>(1)</strong> insufficient robustness in open-set scenarios, where unseen normal motions are frequently misclassified as anomalies, and <strong>(2)</strong> an overemphasis on, but restricted capacity for, local motion reconstruction. To overcome these challenges, we introduce a novel <em>frequency-guided diffusion model with perturbation training</em>. First, we enhance robustness by training a generator to produce perturbed samples targeting the weakness of the reconstruction model. Second, we employ <strong>2D Discrete Cosine Transform (DCT)</strong> to separate high-frequency (local) and low-frequency (global) motion components. Extensive experiments on <strong>five VAD datasets</strong> demonstrate state-of-the-art performance.
      </p>
    </div>
  </div>
</section>

<!-- Motivation -->
<section class="section alt-bg">
  <div class="container is-max-desktop">
    <div class="content-block">
      <h2 class="section-title">üí° Motivation</h2>
      
      <div class="figure-container">
        <div class="figure-wrapper">
          <img src="static/images/intro.png" alt="Motivation illustration"/>
        </div>
        <p class="figure-caption">
          <strong>Figure 1.</strong> (a) Training data consists of seen normal motions; testing data contains unseen normal and abnormal motions. (b) Frequency analysis reveals that motion retaining only 70% low-frequency information remains similar to the original in global structure.
        </p>
      </div>
      
    </div>
  </div>
</section>

<!-- Framework -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="content-block">
      <h2 class="section-title">üèóÔ∏è Method</h2>
      
      <div class="figure-container">
        <div class="figure-wrapper">
          <img src="static/images/framework.png" alt="FG-Diff Framework"/>
        </div>
        <p class="figure-caption">
          <strong>Figure 2.</strong> Overview of FG-Diff. Training includes: (1) minimizing MSE to train the noise predictor, and (2) maximizing MSE to train the perturbation generator. During testing, high-frequency information of observed motions and low-frequency information of generated motions are fused.
        </p>
      </div>
      
      <div class="figure-grid-2col">
        <div class="figure-container">
          <div class="figure-wrapper">
            <img src="static/images/intro_method.png" alt="Method comparison"/>
          </div>
          <p class="figure-caption">
            <strong>Figure 3.</strong> <em>Training:</em> Adversarial training with perturbation generator and denoiser. <em>Inference:</em> DCT separates motion into global (low-freq) and local (high-freq) components for accurate reconstruction.
          </p>
        </div>
        <div class="figure-container">
          <div class="figure-wrapper">
            <img src="static/images/perturb.png" alt="Perturbation Training"/>
          </div>
          <p class="figure-caption">
            <strong>Figure 4.</strong> Perturbation training illustration. (a) Green and yellow points denote original and perturbed motions. (b) The reconstruction domain is extended by perturbation training.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Experiments -->
<section class="section alt-bg">
  <div class="container is-max-desktop">
    <div class="content-block">
      <h2 class="section-title">üìä Experiments</h2>
      
      <h3 class="subsection-title">üèÜ Main Results</h3>
      <div class="figure-container">
        <div class="figure-wrapper">
          <img src="static/images/t1.png" alt="Main results comparison"/>
        </div>
        <p class="figure-caption">
          <strong>Table 1.</strong> Comparison with state-of-the-art methods. Bold: best results; Underlined: second-best; ‚Ä°: best under each paradigm.
        </p>
      </div>

      <h3 class="subsection-title">üî¨ Ablation Study</h3>
      <div class="figure-container">
        <div class="figure-wrapper small">
          <img src="static/images/t2.png" alt="Ablation study"/>
        </div>
        <p class="figure-caption">
          <strong>Table 2.</strong> Robustness analysis of perturbation training. "PT" denotes perturbation training; "Œª<sub>PI</sub>" represents perturbation intensity.
        </p>
      </div>

      <h3 class="subsection-title">üìà Qualitative Results</h3>
      <div class="figure-container">
        <div class="figure-wrapper medium">
          <img src="static/images/vis.png" alt="Anomaly score visualization"/>
        </div>
        <p class="figure-caption">
          <strong>Figure 5.</strong> Anomaly score curves on Avenue and HR-UBnormal datasets. Red circles: abnormal events; Green circles: normal events.
        </p>
      </div>
    </div>
  </div>
</section>

<!-- Demo Videos -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="content-block">
      <h2 class="section-title">üé• Demo</h2>
      <p class="demo-description">Left: ground truth labels. Right: detection results.</p>
      <div class="video-grid">
        <div class="video-item">
          <video autoplay controls muted loop playsinline>
            <source src="static/videos/video_1.mp4" type="video/mp4">
          </video>
        </div>
        <div class="video-item">
          <video autoplay controls muted loop playsinline>
            <source src="static/videos/video_2.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- BibTeX -->
<section class="section alt-bg">
  <div class="container is-max-desktop">
    <div class="content-block">
      <h2 class="section-title">üìö BibTeX</h2>
      <div class="bibtex-container">
        <button class="copy-btn" onclick="copyBibtex()">
          <i class="far fa-copy"></i> Copy
        </button>
        <pre class="bibtex"><code>@article{tan2024fgdiff,
  title={FG-Diff: Frequency-Guided Diffusion Model with Perturbation Training for Skeleton-Based Video Anomaly Detection},
  author={Tan, Xiaofeng and Wang, Hongsong and Geng, Xin and Wang, Liang},
  journal={arXiv preprint arXiv:2412.03044},
  year={2024}
}</code></pre>
      </div>
    </div>
  </div>
</section>

<!-- Footer -->
<footer class="footer">
  <div class="container">
    <p>
      Template adapted from <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
      Licensed under <a href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">CC BY-SA 4.0</a>.
    </p>
  </div>
</footer>

<script>
function copyBibtex() {
  const bibtex = document.querySelector('.bibtex code').textContent;
  navigator.clipboard.writeText(bibtex).then(() => {
    const btn = document.querySelector('.copy-btn');
    btn.innerHTML = '<i class="fas fa-check"></i> Copied';
    setTimeout(() => {
      btn.innerHTML = '<i class="far fa-copy"></i> Copy';
    }, 2000);
  });
}
</script>

</body>
</html>
